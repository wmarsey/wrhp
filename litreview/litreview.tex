\documentclass[a4paper,11pt,twoside,notitlepage]{article}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage[colorlinks=false, pdfborder={0 0 0}]{hyperref}
\usepackage{cleveref} 
\usepackage{fancyhdr}
\usepackage{abstract}
\usepackage{framed}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{parskip}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{appendix}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\usepackage{float}

\usepackage{tikz}
\usetikzlibrary{arrows,chains,matrix,positioning,scopes}
\makeatletter
\tikzset{join/.code=\tikzset{after node path={%
\ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
edge[every join]#1(\tikzchaincurrent)\fi}}}
\makeatother
\tikzset{>=stealth',every on chain/.append style={join},
         every join/.style={->}}
\tikzstyle{labeled}=[execute at begin node=$\scriptstyle,
   execute at end node=$]

\newcommand{\super}[1]{\textsuperscript{#1}}


\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{keywords},
  commentstyle=\color{comments},
  stringstyle=\color{red},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  procnamekeys={def,class}
}

%% BOXED ABSTRACT
\renewenvironment{abstract}
 {
	\small
  	\begin{center}
  	\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  	\end{center}
  	\list{}{
    	\setlength{\leftmargin}{.5cm}%
    	\setlength{\rightmargin}{\leftmargin}%
  	}%
  	\item\relax}
 	{\endlist}

%% OK LETS GO I GUESS
\begin{document}
	\title{Contribution
		\\ \small Literature Review}
	\author{William Marsey
		\\Imperial College London}
	\date{June 2014}
 	\maketitle	

	%% \begin{framed}
	%% 	\begin{abstract}
	
	%% 	Mediawiki's slogan reads: ``Mediawiki: Because ideas
        %%         want to be free''. 

        %%         But qualitatively assessing these articles may be
        %%         something of a moot point. 
	
	%% 	%	Traditionally, a collaborative work may not
        %%            %     disclosed the provenance of its constituent
        %%            %     parts. An individual's stake in the
        %%            %     final work - the valued work - was not
        %%            %     apparent or accounted for in the final
        %%             %    text. Regardless of how clearly demarcated the various
        %%             %    roles in creating the artefact, an individual
        %%             %    collaborator's stake may have been
        %%             %    gauranteed only by way of prior
        %%             %    negotiation. These stakeholder policies were individual
        %%             %    and capricious, and bound the completeness of the work,
        %%             %    the finality (or authenticity) of the
        %%             %    work. 
	%% 	\end{abstract}
	%% \end{framed}
          
        \section{Introduction}
        \subsection{The Research Question}
        Given a collaboratively edited document, and information about
        the collection of edits that constitute that document, may we
        measure the quality of each contribution? And may we use that
        to give all the collaborators a algorithmically-defined 'stake'
        in that final document?
        
        Collaborative work is becoming a big deal. It is both
        interesting and an important trend in modern computer
        use. And the data is abundant. 

        Amongst many other things, this topic is a playground for
        sociology, machine learning, network studies, as well as more
        general studies of conflict, and personality. My work intends
        to focus on the algorithmic side of things - approximate
        string matching in particular. I look at how we may use
        Levenshtein distance, and the various favours,
        varieties and optimizations thereof, to measure contribution
        to a collaborative text, and how we may implement a version of
        this algorithm specifically tailored to our needs.

        The main questions we ask are:
        \begin{itemize}
          \item What does Levenshtein distance define of a
            contribution in the context of massive online
            collaboration?
          \item What are the limitations and implications of defining
            contribution in this way?
          \item What else may we learn from analysing contribution? 
        \end{itemize}

        We base our studies around data from Wikipedia. This study is
        defined by -- and in some ways determined by -- the specific
        context of Wikipedia, but, as we will see, is ultimately
        enriched by it. Due to its open-source nature, and its size,
        studies that touch upon Wikipedia cover a very broad range of
        topics. Many of them are directly related to the topic we
        concern ourselves with here, and many more may enrich our
        study tangentially. 
 
        \section{Previous work}
        There are three sections here for the three different topics
        that come to bear on this subject:
        \begin{itemize}
          \item Wikipedia, studies of wikipedia, and the nature of
            Wikipedia
          \item The 'edit distance problem', Levenshtein distance
          \item The various pre-existing studies that apply the latter
            to the former
        \end{itemize}
        
        \subsection{On Wikipedia}
        Wikipedia's pre-eminence as an online resource is self-evident
        to anyone who has searched the internet for a generic
        topic. The website is ranked 6\super{th} globally in terms of
        website traffic,\footnote{According to Alexa, an Amazon-owned
          company. The statistics are wide-rangingbased on a combined
          measure of Unique Visitors and Pageviews, and the data mined
          from around 25,000 different browser extensions, as well as
          sites that have installed Alexa's
          scripts.\cite{Alexa-about2014} Alexa may well be biased
          towards English speakers and Internet Explorer users, but
          this may underestimate Wikipedia.org's popularity, since
          `two thirds of all Wikipedia articles are in languages other
          than English'\cite{wikimedia-noteonalexa}} and is the
        highest-ranked reference website by far - most of the sites it
        shares the top spots with are portals, search engines,
        shopping mega-sites, and social media
        websites.\cite{Alexa-topsites2014} Despite some skepticism
        (particularly concern over the inherent chaos of the system:
        ``...edits, contributed in a predominantly undirected and
        haphazard fashion by ... unvetted
        volunteers.''\cite{Wilkinson2007}), it is widely
        claimed to be a success, `the best-developed attempt thus
        far of the enduring quest to gather all human knowledge in one
        place'\cite{Mesgari2014}.

        That Wikipedia has become a hub of research in many fields is
        also self-evident to anyone who has searched for articles on
        the subject. Mesgari et al, just quoted, has prepared a very
        recent 'systematic review of scholarly research on the content
        of Wikipedia', which gives an overview of 110 articles on the
        subject --- an attestment to the observation that Wikipedia
        has been 'irresistable point of unquiry for researchers from
        various fields of knowledge', and a useful touching stone for
        this study. Mesgari et al's review finds 82 out of the 100 to
        concern quality in Wikipedia articles, some of these are also
        referenced here, and many of the others will come to bear on
        the study as it progresses.

        Other important general sources will be WikiLit,\cite{wikilit}
        AcaWiki\cite{acawiki} and WikiPapers\cite{wikipapers}, all of
        which are online repositories of academic research into
        Wikipedia and other Wikis (as well as being Wikis
        themselves...).

        \subsubsection{The particularities of Wikipedia, and implications}

        %% As a focal point of many people, the site, as a network,
        %% has been seen as a microcosm of the world wide web on the
        %% whole.[WHERE THE BOLLOCKS HAS THAT PAPER GONE I HAD IT]
        %% Previous work has seen the site reflecting society too, and
        %% has been used in social studies such as Lih's 2004 study of
        %% articles immediately after they have been cited in the
        %% press\cite{Lih2004}, and Metyan's 2012 use of the site to
        %% predict box-office success \cite{Mestyan2012}. These
        %% studies are useful in their exploration of the general
        %% culture affecting Wikipedia.

        %% The question of the `quality' of Wikipedia articles in
        %% general, also pertinent here, has reared its head
        %% regularly. The question is problematic, and papers
        %% concerning it (the earlier ones especially) show concern
        %% about the abandonment of expert opinion, particularly of
        %% the chaos of the system: ``...edits, contributed in a
        %% predominantly undirected and haphazard fashion by
        %% ... unvetted volunteers.''\cite{Wilkinson2007} (The studies
        %% on this are very numerous, and will not be listed here. A
        %% select few are cited throughout this document.)

        The six 'risks' one takes when referencing Wikipedia, as
        defined in an early article on the subject,\cite{Denning2005}
        is a good starting block for identifying the ways in which to
        ragard the 'quality' of content in Wikipedia. We list them
        here, describing the implications for our work with each. Some
        are particular to Wikipedia, some are inherent to all Wikis.
        
        \begin{itemize}
          \item \textbf{Accuracy.} It is important to remember that,
            without severely increasingly the complexity of our
            algorithm, we may not verify the accuracy of
            information. And, if accuracy of information is
            proportionate to value (surely it must be in a reference
            text), then our algorithm may misplace its reward. We may
            most usefully look at the problem as follows. The texts that
            are edited most often are those that are visited most
            often. The previously cited studies of Lih and Mestyan et al
            attest to this - they both study the peaks in activity in
            articles that are brought to attention in some way. We
            find in the work of Bongwon Suh et al that the growth of
            wikipedia is inverse-exponential, as the overheads of
            coordination and beaurocrosy temper content
            creation.\cite{Suh2009}\cite{Kittur2007} Content is more
            likely to be refined and corrected as an article grows
            old.\cite{Wilkinson2007} We can assume, then, that all
            articles tend towards accuracy (we may find this bore out
            in Giles's 2005 semi-formal comparison of Encyclopedia
            Britannica articles to Wikipedia articles - finding an
            average three mistakes in the former and 4 mistakes in the
            latter)\cite{Giles2005}. We may possibly extend this to
            say that all edits improve a text. This is complicated by
            malicious, misinformed or malformed edits, but we will
            discuss dealing with these later.\\
            \textbf{make}
          \item \textbf{Motives.} It has been found that different
            contributors may edit Wikipedia in various different ways,
            according to their proclivities.\\
            \textbf{Response:}
          \item \textbf{Uncertain Expertise.} We may take this to mean
            malformed and misinformed editing, but we may also take it
            to mean malicious editing. As for malicious edits - we
            find a lot of useful information in Potthast et al's work
            on automatic detection of vandals,\cite{Potthast2008} as
            well as the discussions around Wikipedia's own
            Counter-Vandalism Unit (`CVU').\cite{wiki-vandalism}
            including 
          \item \textbf{Volatility.} 
          \item \textbf{Coverage.} Cite that structure is a
            problem.\\
            \textbf{Response:} We may want to reward extra for restructuring.
          \item \textbf{Sources.} There has a been some work
            explicitly taking external links to be relative to
            quality,\cite{CITEHYPERLINKS} and seems to have been bore
            out by a further study which found this to be a heuristic
            used by actual readers.\cite{THISHEURISTICIGUESS}.\\
            \textbf{Response:} We may give give extra weight to the value
            of (working) hyperlinks, and fixing hyperlinks.
        \end{itemize}
            
        \subsection{Wikipedia Self-Valuation}
        \begin{quote}
          [Wikipedia] cannot attain the status of a true encyclopedia
          without more formal content-inclusion and expert review
          procedures.\cite{Denning2005}
        \end{quote}

        

        Most visited articles in an hour - correlates with
        (american-centric) events \cite{wiki-visits}

        Denning says it cannot attain the status of a true
        encyclopedia without more formal content-inclusion and expert
        review procedures\cite{Denning2005} this corroborates by
        findings in \cite{Giles2005}?

        \subsubsection{On Wikipedia}
        `robust and remarkable growth'
        \cite{Kittur2007}\cite{Voss2005} 
        
        Wikipedia, at the last dump, consisted 800G of compressed data
        \cite{wiki-dump}

        \subsubsection{Evaluating Wikipedia articles}
        identify, analyse

        after article mentioned in press \cite{Lih2004}

        compared by 'experts' to 'equivalent' Encyclopedia Britannica articles \cite{Giles2005}

        found metrics of article quality through factor analysis
        \cite{Stvilia2005}

        Analysis by conflict - revisions?\cite{Kittur2007}

        WikiTrust. The most `complete' of the many of the. Exists as
        firefox plugin (though it doesn't work any more) Culmination
        of various studies that try to QUOTE \cite{Adler2007} and QUOTE CITE. It
        was assessed as recently as 2011 \cite{Lucassen2011}
       
        \subsection{Edit difference}

        Standard: Levenshtein distance \cite{Levenshtein1966}
        \begin{figure}
          \begin{tikzpicture}
            \matrix (m) [matrix of math nodes, row sep=4em, column
              sep=4em] { & v_i & \\ v_{i-1} & & v_{i+1}\\ };
            \path[-stealth] (m-2-1) edge node [left]
                 {$d(v_{i-1},v_i)$} (m-1-2) edge node [below]
                 {$d(v_{i-1},v_{i+1})$} (m-2-3) (m-1-2) edge node
                 [right] {$d(v_i,v_{i+1})$} (m-2-3); 
                 \draw[color=red]
                 (m-1-2.north west) -- (m-1-2.north east) --
                 (m-1-2.south east) -- (m-1-2.south west) --
                 (m-1-2.north west);
          \end{tikzpicture}
        \end{figure}
        \cite{Adler2006}

        \subsection{My work in context of these sources}
        different views emerging topics gaps and inconsistencies

        Goals.

        Methods. Python. Levenshtein. Optimisations of.
     
        \section{Conclusions}

        \subsection{Intentions / Progress}
        The Code in appendix A
        
        Possible extensions.  Perhaps we can examine more about what
        we may find out about the articles. Cite other studies Lih's
        2004 study of articles immediately after they have been cited
        in the press\cite{Lih2004}, and Metyan's 2012 use of the site
        to predict box-office success \cite{Mestyan2012}. Lieberman
        worked out their locale.\cite{Lieberman2009}:

        \begin{enumerate}
          \item peaks in activity (rate of levenshtein distance... may
            have to be in a log graph...)
          \item 
        \end{enumerate}

        PREDIFINED / NOT-PREDEFINED ideas of quality. look for when
        the the article levels off? And do this by DATE rather than
        REVISION. We may assume that pageviews are more
        well-distributed than revisions

        summarize major contributions (which do we care about?)
        evaluate your current position point out any flaw in
        methodology/research/contradictions are there any gaps in the
        area which you will cover in your research?  How will you
        integrate sources you have mentioned into your dissertation?
        Point out any areas for further study

        
\clearpage
\begin{appendices}
\section{Appendix A: Code progress}
\subsection{Python class for scraping a Wikipedia article's revision history}
The following code is a first draft of a class which incrementally
traces, parses, and stores the revision history of select articles. It
chooses random articles up to a maximum amount of articles unless an
article (or articles) are specified. It traces the entire
discoverable\footnote{Using the Wikipedia API, articles can either be
  traced back to their origin (revision parent ID = 0), or to the
  point at which a loop is found in the revision history - this
  usually happens with older articles.} history of the, unless a
specific depth is specified by the user.

The class already yields workable data, but here is some immediate
further work for this code:
\begin{itemize}
  \item Allow the user to specify timeframe
  \item Allow for integration with a postgres database (at the moment
    the code saves the data in CSV format).
\end{itemize}

This code is an extension to an existing wikipedia API class for
python (which did not provide the features we need
here).\cite{python-wikipedia}

\lstinputlisting[language=Python,frame=single]{../wikiScraper/WikiRevisionScrape.py}
\subsection{Example output}

\clearpage
\section{Appendix B: Python class for basic, space-naive Levenshtein
  implementation}
\subsection{Code}
\lstinputlisting[language=Python,frame=single]{../lshtein/basic/LevDistBasic.py}
\clearpage
\subsection{Example output}
\lstinputlisting[language=sh,frame=single]{../lshtein/basic/output.txt}
\end{appendices}

\clearpage
\renewcommand\refname{Bibliograpy} \bibliography{litreview}{}
\bibliographystyle{plain}
\end{document}
