\documentclass[a4paper,11pt,twoside,notitlepage]{report}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage[colorlinks=false, pdfborder={0 0 0}]{hyperref}
\usepackage{cleveref} 
\usepackage{fancyhdr}
\usepackage{abstract}
\usepackage{framed}
\usepackage{cite}

\renewenvironment{abstract}
 {
	\small
  	\begin{center}
  	\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  	\end{center}
  	\list{}{
    	\setlength{\leftmargin}{.5cm}%
    	\setlength{\rightmargin}{\leftmargin}%
  	}%
  	\item\relax}
 	{\endlist}


\begin{document}
	\title{Chao Wu
		\\ \small Literature Review}
	\author{William Marsey
		\\Imperial College London}
	\date{June 2014}
 	\maketitle
	

	\begin{framed}
		\begin{abstract}
			Traditionally, a collaborative work may not
                        have itself disclosed the provenance of its constituent
                        parts. An individual's stake in the
                        final work - the valued work - was not
                        apparent or accounted for in this
                        text. Regardless of how demarcated the various
                        roles in creating the artefact may have
                        been, the individual's part may have been
                        gauranteed only by way prior
                        negotiation. These policies were individual
                        and capricious, and bound the completeness of the work,
                        the finality (or authenticity) of the
                        work. 

                        But it is exactly these foundations that
                        have been severely undermined in the last few
                        decades, by the revolutions and eveloutions
                        apparent in the development of modern
                        content-creation. 'Finality' of a work has no
                        real meaning in the context of a Wikipedia
                        article, as does ownership become a very
                        problematic concept. The emergence of
                        such transparent, massively-distributed
                        participation is just one more consequence of
                        that urge to connect that brought us the
                        internet itself, and its manifestation has
                        brought about some decidedly awkward
                        reconsiderations of concepts
                        that, in the last few centuries at least, have
                        been very important in consideration of 'the
                        work': ownership, accountability and
                        authenticity are just a few.

                        This project examines the status of valuation
                        in this context, with an emphasis on the
                        algorithms that may be used to seek out value
                        in large data-sets, and histories of online
                        collaboration.
		\end{abstract}
	\end{framed}

        \section{Open Source Valuation}
        \subsection{Previous Work}

        \subsection{McGuinness, et al.}
        Evaluating articles according to internal and external
        links. This may be an appropriate strategy, as one study has
        shown this to be a natural heuristic for the average Wikipedia
        reader.~cite{Lucassen2010}

        \subsubsection{WikiTrust}
        A benchmark piece of software is WikiTrust. It was developed
        in response to Wikipedia reaching a milestone of maturity
        (having by 2006 collected content comparable to Encyclopedia
        Britannica [CITE GILES 2005]), and designed to evaluate each
        word of an article for its 'trustworthiness'. It also
        developed out research made by others, and incorporated a few of
        the research and heuristics gathered by others, such as the
        [SUMMARY LATER] of Kittur, et al., and the [SUMMARY LATER] of
        Cross. 

        [CRITICISMS in Luyt et al.]

        This trust is expressed as a real number between 0 and 1, and
        calculated using the 'age' of the word (each edit that word
        survives is regarded implicit attestment to worth on behalf of
        the editor), and the reputation of the contributor of that
        word. Conversely, the reputation of a contributor is
        calculated using the survival rate of his words. The 'trust'
        of each word is then represented as a grade of yellow,
        highlighting the word as we see below - the darker yellow
        parts are the most trusted parts.

        [DIAGRAM: WIKITRUST IN ACTION OMG]

        The software, although a little aged now (the last update was
        in 2008), tackles a lot of the concepts we concern ourselves
        with here, as well as guiding us to solutions to a few of the
        problems we will come accross, such as different kinds of
        malicious input. There has been a critical evaluation of it as
        recent as 2011,~/cite{Lucassen2011}. 

        [TALK ABOUT HOW TRUST IS ONLY ONE PART OF VALUE]

        [TALK ABOUT how lucassen found the software to be kind of
          useless, but a nice thing anyway, but basically useless, lol]
 
	\section{Tackling the Edit Distance Problem}
        Much of the greatest algorithm discoveries in this area come
        out of work made in the 60s and 70s. We find Levenshtein's
        work, and his eponymous edit distance algorithm to be the nexus of
        this work~/cite{Levenshtein1966} - much of the work in the 50 years since the
        publication of Levenshtein's original work has been working
        towards confirming and improving this original algorithm.~/cite{Navarro2001}

        This family of algorithms tackle the problem in a similar way
        - comparing each of the characters of two strings, and
        creating some sort of table with which to speed up computation
        of edit distance. These improvements are manifold, but
        cluster around two principles: disregard rows and columns of
        the table that we know not to improve the final answer;
        compute the comparisons in a way that leverages the computer's
        natively quick operations (i.e. bit-wise comparisons,
        bitmapping representations of the strings, etc).

        The algorthims are principally Dynamic programming algorithms
        - the space complexities for most are usually the limiting
        factors in terms of implementation. Navarro notes that one of
        the quickest algorithms in this area is 'only of theoretical
        interest', having a space requirement [OF LOADS].

        \subsection{Intentions}

\bibliography{litreview}{}
\bibliographystyle{plain}	
\end{document}
