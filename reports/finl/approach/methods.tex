\section{Methods}
From our background work, we take a three step method of analysing the
article.

Firstly, we must take the most recent version of an article and take
it as a static, `finished' version of the collaboration. We will
discuss later the possibility of predicting future versions of the
article.

Secondly, we measure the distance between each article version using a
levenshtein distance, but splitting the text up into text species.

Thirdly, we measure the levenshtein distance between each revision in
the article's history

%% \begin{itemize}
%% \item \textbf{We assume that the final, or `target' article is of
%%   `good' quality.} There are many studies which concern themselves
%%   with verifying the accuracy and quality of Wikipedia articles --- In
%%   this study we are specifically concerned with the quality of
%%   contribution, i.e. the quality of text within the article, relative
%%   to the domain of the article. Here, the quality of the article
%%   itself is a moot point.
%% \item \textbf{We assume the article is well-formed.} As much as we do
%%   not concern ourselves with article accuracy, we also assume that the
%%   Wikipedia markup is also well-formed. We may also eventually check
%%   whether links are invalid, but for the moment we assume that they
%%   are.
%% \item \textbf{We make no distinction between humans, bots and
%%   anonymous editors.}
%% \end{itemize}

\section{Data collection and storage}
\subsection*{The WikiMedia API}
\label{sec:wiki-api}
WikiMedia's API service provides simple access to a wiki data,
features and meta-data over HTTP,\cite{wiki-api} and for this project,
provides the entirety of our data. In this section we explore the
process of collecting Wikipedia data, the peculiarities of Wikipedia
as a data source, and the algorithmic caveats necessary to deal with
them.

Our basic request is simple: we send a HTTP request to a given wiki
site's `/api.php' file, sending the query parameters a
`prop=revisions', `rvprop=content', and `titles=X|Y' to get those
page's most recent revision contents. We can prefetch a title using a
random request (parameters `list=random\&rnlimit=1'), and in each case
we can add the `format=json' parameter, and quite easily parse the
results. 

To trace the history of a given page, then, we need only augment the
`rvprop' argument to include `ids' (`rvprop=content|ids') in order to
discover the parent id and trace the history backwards from there. The
procedure defined in algorithm~\ref{alg-data} demonstrates this
clearly, showing the child-parent swap at line~\ref{datal2}. The
condition on the while loop (line~\ref{datal1}), however, shows the
first oddity with the Wikipedia histories. Most articles will
terminate at their origin, showing parentid 0. Some, instead, enter a
terminal cycle, with the oldest fetched revision giving its parent to
be a much younger version of the same article. 

%%%% Data fetching algorithm
\begin{algorithm}
  \caption{Data fetching}\label{alg-data}
  \begin{algorithmic}
    \Procedure{Fetch}{$pageid$}
    \State $corrupt \gets \emptyset$
    \State $visitedpages \gets \emptyset$
    \State $revid \gets 0$
    \State $parentid \gets wiki.getlatest(pageid)$
    \While{$revid \ne 0$ AND $revid \notin visitedpages$}\label{datal1} 
    \If{$revid$ is in the database}
    \State $parentid \gets database.getparentid(revid)$
    \Else
    \State $pagedata \gets wiki.getpage(revid)$
    \EndIf
    \If{$pagedata$ is corrupt}\label{datal3}
    \If{corruptness is within recoverability bounds}
    \State $corruptpages \gets corruptpages + (revid, parentid, domain)$
    \Else
    \State terminate fetch
    \EndIf
    \Else
    \State $database \gets page data$
    \EndIf
    \State $visitedpages \gets visitedpages + (revid, domain)$
    \State $revid \gets parentid$\label{datal2}
    \EndWhile
    \ForAll {$(revision, parent, domain) \in corrupt$}
    \State $CorruptClean(revision, parent, domain)$\Comment{See
      algorithm~\ref{corrupt-clean}}
    \EndFor
    \State Mark $pageid$ as complete in $database$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

We must do is keep a track of all the revisions we already know exist
of that article, so that we may identify these cycles, and terminate
the fetch loop early. In early tests, we find that these cycles occur
exclusively amongst older articles (though not all old articles have
the problem). For the purposes of this study, then, we must
acknowledge that any `complete' history of an article in our databases
is in fact only the complete \textit{discoverable}
history.\footnote{We may note that the Wikipedia site warns that some
  histories are only part-discoverable. Using our fetch algorithm we
  have managed to often surpass the boundaries described on Wikipedia,
  though the cycle problem does occur at a time approximate to that
  mentioned on the Wikipedia. We may assume that some server
  corruption occurred at around that point in Wikipedia's history.}

A second notable problem was the regular corrupt values returned by
Wikipedia. We see that we test for corruption in at line~\ref{datal3},
storing a list of corrupt pages - often the data is returned with
missing entries, but was often missing data that was pertinent to
later study - namely, timestamp and revision content. For ease of
analysis later, we choose to `circumnavigate' corrupt entries in the
database, changing pointers between children and parents in situ. We
may then later trace a history of incorrupt entries using these
pointers. The procedure for this is detailed in
algorithm~\ref{corrupt-clean}.

Finally, we define some corruptness of data to constitute a fetching
failure. With our model, we only do this when the parentid is
missing. Though this does not happen often, to handle these cases we
mark a pageid as having a succesful fetch by adding it to a special
table in the database. This can also be useful for fetches that are
interrupted in other ways, as with hardware and network problems. This
table is found in figure~\ref{database-schema} as `wikifetch'. 

\begin{algorithm}
  \caption{Corrupt pages}\label{corrupt-clean}
  \begin{algorithmic}
    \Procedure{CorruptClean}{corruptrev, parent, domain}
    \State $childrev \gets database.getchild(corruptrev)$
    \State $database.setparent(childrev, parent)$ \Comment{Now the
      corrupt revid is circumnavigated}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection*{The database}
The database is implemented in PSQL, and accessed via a python package
leveraging the psycopg library. The package is used by all the other
classes used in this project and provides simple inserting, changing,
fetching and checking of data. The only more complex operation is the
datadump function. The operation is more specific than the other
functions, and may have been pieced together using these functions,
but in implementation it was much quicker to correlate and fetch data
using the SQL `JOIN' statements, rather than multiple fetches in a
Python for loop, for example.

The database relies upon the uniqueness of revision ID and domain-name
pairs to function. The database schemata can be found in
figure~\ref{database-schema}. These databases are referenced at all
points of this project.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.3\linewidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      \underline{revid} & \underline{domain} & content\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{Table: wikicontent}
  \end{subfigure}
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \begin{tabular}{cc}
      \toprule
      \underline{pageid} & \underline{domain} \\
      \midrule
      $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{Table: wikifetched}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      \underline{revid1} & \underline{revid2} & \underline{domain} & distance\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikitrajectory}
  \end{subfigure}\\
  \vspace{10 mm}
  \begin{subfigure}[b!]{\linewidth}
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      \underline{revid} & \underline{domain} & pageid & title & username & userid & time & size &
      comment \\ 
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$
      & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikirevisions}
  \end{subfigure}
  \vspace{30 mm}
  \begin{subfigure}[b!]{\linewidth}
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      \underline{revid} & \underline{domain} & maths & citations & filesimages & links &
      structure & normal & gradient\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &
      $\vdots$ & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikiweights} 
  \end{subfigure}
  \caption{Schemata for the database used to store wikipedia data}
  \label{database-schema}
\end{figure}

%%WEIGHTING BY MARKUP
\subsection*{Text species}
Using the wikimedia markup system, we may easily identify certain
species of text. They are as follows:
\begin{itemize}
\item Internal links
\item External links
\item Images
\item Files
\item Musical scores
\item Math-formatted text (similar to Latex math environment)
\item Section headings (of differing levels)
\item `Citation Needed' tags
\item `As of' tags (used for identification of age-sensitive
  information)
\item Block quotes
\item Tables
\end{itemize}

We then group them logically, as follows:
\begin{description}
  \item[Equation]\hfill\\
    Math-formatted text
  \item[Source validation]\hfill\\
    Block quotes, Citations, `Citation Needed' tags, `As of' tags
  \item[Links]\hfill\\
    Internal Links, External Links
  \item[Structural]\hfill\\ 
    Section headings, Tables\\
    It was found in 2005 that this, if anything, was the clearest
    difference between Wikipedia and commercial
    encyclopedias,\cite{Giles2005} supporting previous
    conjecture.\cite{Denning2005} The regexes in this group provide a
    simple way of noticing changes that affect structure.
\end{description}

We ma split the string along the boundaries of these markup tags, and
send levenshtein distance calculator sperate strings.
FINISH

%%DENSITY
\subsection*{Undone and partially undone operations}
We consider three different ways of classifying an edit as valueless,
or partially valueless. Two ways of classifying these kinds of edit
are found in previous research, and are covered in figure
\ref{fig:undo}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes, row sep=4em, column
      sep=4em]{ & v_i & \\ v_{i-1} & & v_{i+1}\\ };
    \path[-stealth] (m-2-1) edge node [left]
         {$ed(v_{i-1},v_i)$} (m-1-2) edge node [below]
         {$ed(v_{i-1},v_{i+1})$} (m-2-3) (m-1-2) edge node
         [right] {$ed(v_i,v_{i+1})$} (m-2-3); 
  \end{tikzpicture}\\
  \textit{case a)} if $ed(v_{i-1},v_i) < ed(v_{i-1},v_i) +
  ed(v_{i},v_{i+1})$, then $ed(v_{i-1},v_i)$ has been
  partially undone.\\ 
  \textit{case b)} if $ed(v_{i-1},v_{i+1}) = 0$
  ($v_{i-1} = v_{i+1}$), then $ed(v_{i-1},v_i)$ has been
  completely undone.
  \caption{Diagram showing identification of a partially or
    completely undone edit}
  \label{fig:undo}
\end{figure}

Figure \ref{fig:undo} shows how we may identify undone or partially
undone edits, when they are undone immediately, as layed out in Adler
et al.\cite{Adler2007} The triangle represents three consecutive
revisions, and the arrows are the edit operations that transform one
to another. By calculating the edit distance between the distance
versions $v_{i-1}$ and $v_{i+1}$ we may characterise a longer history
of revisions than usual, and use that context in order to
re-characterise the edits it encompasses. In the figure above, case
$a$ describes $v_i$ as a 'diversion'. If $v_{i-1}$ can be transformed
into $v_{i+1}$ with less operations than the two edits that actually
bridged that gap, then perhaps some of the edits in $v_{i-1}
\rightarrow v_i$ were unnecessary, and undone by $v_{i+1}$. In this
case, we may punish the edit $v_{i-1} \rightarrow v_i$ (for the
diversion), reward $v_{i} \rightarrow v_{i+1}$, or both. Case $b$ is
an extreme version of $a$ --- the texts $v_{i-1}$ and $v_{i+1}$ are
identical, so the changes in $v_i$ must have been completely
undone. These reverts are common in normal Wikipedia edit
practice.\cite{wiki-revert}

\begin{floatingfigure}[p]{0.4\textwidth}
  \centering
  \pgfplotsset{width=0.4\textwidth}
  \begin{tikzpicture}
    \begin{axis}[
        title={Dummy revision history},
        ylabel={Ed. distance from final},
        xlabel={revision ID},
      ]
      \addplot table {dat/dummy_history.dat};
    \end{axis}
  \end{tikzpicture}
  \caption{Graph showing a `trajectory plot'}
  \label{fig:dummy-history}
\end{floatingfigure}

This algorithm, however, is limited in its scope, and we may
come across situations where reversions occur over a series of
edits. Although the system may easily be extended to cover
larger spans of history, to consider many nodes would require
the edit-distance computation of very many different node
pairs. 

Let us propose a more efficient way of characterising redundant
entries in terms of longer history spans. We must utilise the fact
that we have take one article to be the ultimate destination of all
previous edits in order to do so. Let us graph the entirety of a
wikipedia's revision history in terms of the edit distance from this
final version (see figure~\ref{fig:dummy-history}).

Each point represents a different version, it's coordinates being its
timestamp, and the edit distance from that revision to the final
revision. A line between two points represents the edit-distance
between each version. Given this information, we may consider only
those revisions that bring us closer to the final version. They appear
in figure~\ref{fig:traj-preen-1} as lines with a negative gradient
(blue); those with a positive gradient take us further away from the
final version (red).

\begin{floatingfigure}[p]{0.4\textwidth}
  \centering
  \pgfplotsset{width=0.4\textwidth}
  \begin{tikzpicture}
    \begin{axis}[
        title={Dummy revision history},
        ylabel={Ed. distance from final},
        xlabel={revision ID},
      ]
      \addplot [blue] table {dat/dummy1.dat};
      \addplot [red] table {dat/dummy2.dat};
      \addplot [blue] table {dat/dummy3.dat};
      \addplot [blue, only marks] table {dat/dummy4.dat};
    \end{axis}
  \end{tikzpicture}
  \caption{Trajectory plotting: a technique for preening results}
  \label{fig:traj-preen-1}
\end{floatingfigure}

We need not compute edit distances with a positive gradient (the two
red lines). This is simple to implement. Given a version $rev_i$, having
computed it's immediate edit distance $ed(rev_{i-1},rev_i)$, and it's edit
distance from the final version, $ed_{final_i}$, we know our next
computation must be $ed(rev_{j-1},rev_j)$, such that $j$ is the smallest
number that satisfies the qualities $j > i$ and $ed_{final_j} <
ed_{final_{j-1}}$.

Another possible strategy would be to disregard all edit distances
that, at any point, lie between two versions that are further away
from final version than the version currently being considered.

\begin{floatingfigure}[p]{0.4\textwidth}
  \centering
  \pgfplotsset{width=0.4\textwidth}
  \begin{tikzpicture}
    \begin{axis}[
        title={Dummy revision history},
        ylabel={Ed. distance from final},
        xlabel={revision ID},
      ]
      \addplot [blue] table {dat/dummy1.dat};
      \addplot [red] table {dat/dummy5.dat};
      \addplot [blue] table {dat/dummy6.dat};
      \addplot [blue, only marks] table {dat/dummy4.dat};
      \fill [green!25,fill opacity=0.5] (axis cs:4,13) rectangle (rel axis
      cs:1,0);
      \addplot[red,dashed,update limits=false] 
      coordinates {(-2,13) (14,13)};
      \addplot[red,dashed,update limits=false] 
      coordinates {(4,-2) (4,23)};
    \end{axis}
  \end{tikzpicture}
  \caption{Trajectory plotting: a more lossy approach}
  \label{fig:traj-preen-2}
\end{floatingfigure}

In figure~\ref{fig:traj-preen-2}, $rev_i$, the currently considered
version, is represented by the node at (4,13). The green rectangle
shows the area in which we look to find $rev_j$, and the blue lines
are those edit distances we both to compute. In this case, after
considering $rev_i$, we move to the $rev_j$ such that $j$ is the
smallest number that satisfies to qualities $j > i$ and $ed_{final_j}
< ed_{final_i}$. In this graph above we move from (4,13) to (8,12).

It is worth stating here that, in both these strategies, after
discovering $rev_j$, we always compute $ed(rev_{j-1}, rev_j)$ rather
than $ed(rev_i,rev_j)$. 

\begin{floatingfigure}[p]{0.4\textwidth}
  \centering
  \pgfplotsset{width=0.4\textwidth}
  \begin{tikzpicture}
    \begin{axis}[
        title={Dummy revision history},
        ylabel={Ed. distance from final},
        xlabel={revision ID},
      ]
      \addplot [blue] table {dat/dummy1.dat};
      \addplot [red] table {dat/dummy7.dat};
      \addplot [blue] table {dat/dummy6.dat};
      \addplot [blue, only marks] table {dat/dummy8.dat};
      \fill [green!25,fill opacity=0.5] (axis cs:4,13) rectangle (rel axis
      cs:1,0);
      \addplot[red,dashed,update limits=false] 
      coordinates {(-2,13) (14,13)};
      \addplot[red,dashed,update limits=false] 
      coordinates {(4,-2) (4,23)};
    \end{axis}
  \end{tikzpicture}
  \caption{Trajectory plotting}
  \label{fig:traj-preen-3}
\end{floatingfigure}

However, with this technique we find some limitations -- we may find a
history such as the one found in
figure~\ref{fig:traj-preen-3}. Between time-points 5 and 6, we find a
downward trajectory, and we may infer here that the editor has altered
the article so as to more closely approximate the final
version. Perhaps we should not simply disregard this edit simply
because it is undone in the next revision. 

Also, perhaps we may give some lessened reward to edits that move away
from the final version, rather than disregarding them altogether. We
may imagine quite easily that the work done here may have inspired
`correction', or at least be part of an ongoing discussion which moves
devlopment of the article on if not towards its final version. %%LESS
%%                                                                %%BULSHIT
%% \begin{floatingfigure}[p]{0.4\textwidth}
%%   \centering
%%   \pgfplotsset{width=0.4\textwidth}
%%   \begin{tikzpicture}
%%     \begin{axis}[
%%         title={Dummy revision history},
%%         ylabel={Ed. distance from final},
%%         xlabel={revision ID},
%%       ]
%%       \addplot [blue] table {dat/dummy1.dat};
%%       \addplot [red] table {dat/dummy7.dat};
%%       \addplot [blue] table {dat/dummy6.dat};
%%       \addplot [blue, only marks] table {dat/dummy8.dat};
%%       \fill [green!25,fill opacity=0.5] (axis cs:4,13) rectangle (rel axis
%%       cs:1,0);
%%       \addplot[red,dashed,update limits=false] 
%%       coordinates {(-2,13) (14,13)};
%%       \addplot[red,dashed,update limits=false] 
%%       coordinates {(4,-2) (4,23)};
%%     \end{axis}
%%   \end{tikzpicture}
%%   \caption{Trajectory plotting: An unfair example}
%%   \label{fig:traj-preen-3}
%% \end{floatingfigure}


So, we calculate a `gradient factor', which we may use to modify the
reward given to an edit. Taking the trajectory distance as $\Delta y$,
and the time difference between that revision and it's predecessor as
$\Delta x$, we calculate a real number between 0 and 1 using the
function in~\ref{fig:circle-map}. We map every point on a the shown
arc to a our gradient factor. The real number increases the more
acutely the article approaches it's final version: inserting a lot of
text that is eventually deleted results in a small gradient factor,
and including a lot of text that is also in the final text results in
a number closer to one. The time frame of this change alters the
number also -- the sooner an edit is made after the previous one, the
closer the gradient factor will be towards either 0 or 1, depending on
the sign of $\Delta y$. (Note that the gradient factor a negative
$\Delta x$ is undefined, as a revision cannot occur before it's
predecessor.)

%% Our most important algorithm, however, is the trajectory calculation
%% algorithm. With this algorithm we allow ourselves to automatically
%% identify some of the context of the revision history, using the same
%% tools with which we analyse pair distance.

%% We consider that, although Wikipedia articles have no endpoint as a
%% rule -- we refer to page~\ref{quote-page} -- we must take the most
%% recent page in our history sample as a kind of `goal'. We measure
%% every revision's levenshtein distance from the final version, giving
%% us an idea of how close to `finished' it is, at least in terms of this
%% history. We then, taking that levenshtein distance as $\Delta y$, and
%% the time difference between that revision and it's predecessor as
%% $\Delta x$.

\begin{figure}
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \[gfactor(\Delta x,\Delta y) = \left\{ 
    \begin{array}{l l l}
      1 & \quad \text{if ${\Delta}x = 0$ and ${\Delta}y < 0$ }\\
      0 & \quad \text{if ${\Delta}x = 0$ and ${\Delta}y >= 0$ }\\
      \frac{arctan({\Delta}y/{\Delta}x)}{\pi}\quad\text{if ${\Delta}x > 0$}
    \end{array} \right.\]
    \caption{Gradient factor definition}
  \end{subfigure}

  \vspace{10mm}

  \begin{subfigure}[b]{\linewidth}
    \centering
    \begin{tikzpicture}[x=1.25cm,y=1.25cm]

      \draw[thick,red,dashed] (4,0) -- (4,1);
      \draw[thick,red,dashed] (4,7) -- (4,8);
      \draw[thick,red,dashed] (7,4) -- (8,4);
      \draw [green] (4,4) circle (3cm);
      \node[fill,blue,circle] at (4,4) (o) {};
      \node[fill,blue,circle] at (7,4) (a) {};
      \node[fill,blue,circle] at (6,2) (b) {};
      \node[fill,blue,circle] at (6,6) (c) {};
      \node[fill,blue,circle] at (4,1) (d) {};
      \node[fill,blue,circle] at (4,7) (e) {};
      
      \node[fill=white,left=1.5cm of o, minimum height=3cm]{\textit{undefined region}};
      \node at (6,7) {\textit{moving away from final}};
      \node at (6,1) {\textit{moving towards final}};
      
      \draw [thick, blue, ->] (o) -- (7,4) node[sloped, midway, above]{$0.5$};
      \draw [thick, blue, ->] (o) -- (6,2) node[sloped, midway, above]{$0.75$};
      \draw [thick, blue, ->] (o) -- (6,6) node[sloped, midway, below]{$0.25$};
      \draw [thick, blue, ->] (o) -- (4,1) node[midway, right]{$1$};
      \draw [thick, blue, ->] (o) -- (4,7) node[midway, right]{$0$};
    \end{tikzpicture}
    \caption{Gradient factor definition: a visual representation}
  \end{subfigure}
  \caption{Mapping of trajectory angle from a single revision point to
    gradient factor}
  \label{fig:circle-map}
\end{figure}

The significance of this number is two-fold -- it at once describes
the amount of change created in the history in terms of the target of
that history, and the longevity of the article's previous state,
(i.e. it's stability). For an edit to approach 1, it needs not only to
contribute a large proportion of the text's final form, but also to do
so very quickly after the previous revision. Later we will discuss
what we can understand from these values in isolation, and how they
relate to one another as a set. 

\subsection*{Visualisation}
Given that we are distributing wealth according to a series of weight
factors, it may be useful to devise a system that visualises how these
weight factors affect distribution. This would simplybe a matter of
expressing the final `score' in terms of these variables, and
producing some interactive graphs. Several good attempts have been
made to visualise Wikipedia history data, with varying levels of
success,\cite{Chi2008}\cite{Sabel2007}\cite{Suh2007}\cite{Wu2013}\cite{Viegas2004}
so the work would be well supported. We will look into the viability
of this extension further into the project.

\subsection*{Further analysis}
Given the over-arching nature of Wikis, it may be possible to derive
other information about the articles we study. Past studies have used
revision histories in order to figure out a variety of different
things, including a 2012 study which used it to predict box-office
success,\cite{Mestyan2012} a 2009 study that was able to geo-locate
editors by their edits.\cite{Lieberman2009}

Given the by-products of our study, we may be able compare and
contrast different categories of article, or editor. It would be
interesting to contrast the actions of humans, and bots,\footnote{It
  has been noted that there are around 700 bots registered on
  Wikipedia (as of 2014). Though not all of them make edits, those
  that do are very prolific, and are known to reverse malicious
  submisions in a matter of seconds.\cite{wiki-bots}\cite{bbc-bots}}
and perhaps look at the nature of edits made by different groups of
editors.

\section{Procedures}
The data we collect via the Wikipedia API goes through a series of
procedures in order to extract measurements of it.

%pair comparison / weighted distance
First we compare the difference between the child-parent pairs of
revisions. This process is fairly simple, and is described in
algorithm~\ref{pair-comp}. The only special condition we introduce
here is comparing the oldest revision with an empty string.
