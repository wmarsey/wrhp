\section{Technologies}
So far I have written two Python classes, which, together can fetch an
entire history of a wikipedia article, and compute Levenshtein
distances between the texts.

WikiRevisionScrape is a Python class which harnesses the Wikipedia API
in order to download various pieces of information about articles and
their histories. It is inspired by open Wikipedia metadata classes
such as 'Wikipedia Miner'\cite{wiki-miner}, or the revision-fetching
'Java Wikipedia Library / Wikipedia Revision
Library'.\cite{wiki-java}\cite{Ferschke2011}

If the user doesn't specify a particular article title, we choose a
random one, and trace it's history back. We can also set a parameter
in order to pick a random article multiple times. There are various
ways of improving the efficiency of this program (such as requesting
multiple pages at once), and this will be introduced in a later
version. At the moment the scraper is fully functional, though at the
moment saves the data in CSV files. I will change this so that it uses
a postgres database. \textbf{Code and example output can be found in
  appendix \ref{wiki-scrape} starting page \pageref{wiki-scrape}.}

Another python class, LevDistBasic, is a naive implementation of
Levenshtein distance (no space or speed optimisations). It is a
first-attempt implementation of the algorithm, and it can return the
Levenshtein distance, the computation table, the edit operation (in
two different formats --- human readable, and as a list of
tuples). Future changes are many, such as including weightings for
different kinds of string, more space-efficient and speed efficient
implementations, etc. \textbf{Code and example output can be found in
  appendix \ref{levenshtein-implement} starting page
  \pageref{levenshtein-implement}.}

These two classes can be entwined manually, by fetching two pieces of
data from the former and feeding them into the latter. The next step
will be to build a class which autmatically builds a database of
files, calculating and storing distances as it does so, but there is
no point in writing such a class until I change the way the scraper
deals with the data it fetches (it should instead returned at the end
of a function --- the use of CSV files is a temporary hack).

I choose Python principally for the ease at which it handles and
passes around different kinds of data. However, even the optimised
versions of the algorithms I will use will be fairly slow. If speed
becomes an issue I will rewrite my code into C++, but since
speed-efficiency is not really the goal of this project, this may not
be a consideration.
