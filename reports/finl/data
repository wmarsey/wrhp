Though we initially approached Wikipedia as a freely available and
appropriate data source, it came to be clear that the peculiarities of
the data would shape this study. Eventually the problems encountered
while using the data would reveal unseen problems in the question this
project put forward.

The Wikimedia software used to create Wikipedias - a php system -
comes with its own publicly accessable API, and this is where the
project began. There are some existing python libraries that wrap this
API, but I chose to write my own for the purposes of this
project. This library concentrates on tracing revision history much
more than existing software does. Here is the main outline of the process of
downloading an article's history:


Algorithm for fetching an article's entire revision history
------------

wiki = Wikipedia() //connection to wikipedia API

page = wiki.request_random_page().json() //return page details as json
store_to_database(page)

parentid = page['parentid'] //this detail returned in json

while parentid is not 0:
      page = wiki.request_by_revid(parentid).json()
      
      store_to_database(page)

      parentid = page['parentid']

------------



--Cyclical revision histories--
This algorithm worked in some cases, but a problem cropped up with
pages that were more than around 10 years old. Once approaching the
2003 mark, some page histories would suddenyl refer to themselves. The
parentid would point to a much younger revision ID, and the program
above would run eternally. Explanations for this have not been
forthcoming - online discussions are spare. We believe that Wikipedia
may have used a different numbering system at some point, or that the
details were corrupted at some point. However, this doesn't quite
explain why the page would point to an earlier version of its on
history exactly, rather than a random number. 

In any case, we had to change our tact. Instead of fetching an
article's entire history, we now look for the entire discoverable
history:



Algorithm for fetching an article's discoverable revision history
------------

wiki = Wikipedia() //connection to wikipedia API

page = wiki.request_random_page().json() //return page details as json
store_to_database(page)

parentid = page['parentid'] //this detail returned in json
visited = [parentid]

while parentid is not 0 and parentid is not in visited:
      page = wiki.request_by_revid(parentid).json()
      
      store_to_database(page)

      parentid = page['parentid']
      visited.append(parentid)

------------



--Corrupt entries--
The second algorithm was much more useful than the first, but would
throw errors further down the line. The data stored was sometimes
missing parts. Given the development of the rest of the project, we
had to define what constituted a 'useless' entry. Our later analyses
stipulated that we must have access to the following:
	   - Revision text
	   - Author
	   - Timestamp
	   - Revision ID
	   - Parent ID
Therefore we had to change the algorithm again, allowing for
rejection:


Algorithm for fetching discoverable revision history, allowing for
corrupt entries
------------

wiki = Wikipedia() //connection to wikipedia API

corrupt = True
while corrupt:
      page = wiki.request_random_page().json() //return page details as json
      if valid(page):
      	 store_to_database(page)
	 corrupt = False

parentid = page['parentid'] //this detail returned in json
visited = [parentid]
corrupted = []

while parentid is not 0 and parentid is not in visited:
      page = wiki.request_by_revid(parentid).json()
      
      if valid(page):
      	 store_to_database(page)
      else:
	 corrupt.append(parentid)

      parentid = page['parentid']
      visited.append(parentid)

for each id in corrupted:
      circumnavigate_in_database(id)      

------------



We may notice here that the parentid is assumed to be present - there
has not been a case yet where it has not been, though text, timestamp
and author has been missing quite a lot. Later down the line we needed
to be able to trawl through our local database using those
child-parent ID links, so we had to change those pointers in the
database to avoid the missing entries.

The curious thing about these corrupt entries was that they were
discoverable on the wikipedia website, with all details
intact. Perhaps the public API uses a different data store than that
which is used internally. In any case, we are brought to another major
problem:

--Generally unreliable API
Fetching data for analysis seemed to be a fairly benign process, so we
tried numerous ways of speeding it up. Noticing that we could save
time by reducing our HTTP requests, we tried leveraging the following
feature of the Wikimedia API:

<> QUOTE ABOUT BEING ABLE TO FETCH 500 <>

As we can see from the appendix <>, many of the articles we fetched
were below 500 revisions, so this feature could be used to fetch many
of our articles in just one HTTP request.

However, we found that the Wikipedia API would consistently much less
than 500 versions, returning a different number of version each
time. Example output below:

<>fetch, count, print, show that less that 500, different numbers each
time<>

--Extra features--
Generally, though, the algorithm was sufficient. In the end we also
implemented the following extra features:
- Random language selection
  The algorithm selects a Wikipedia at random from the hundreds of
  languages available. This can be overriden
- Depth limits
  The algorithm automatically fetches to maximum depth, but this can
  be limited to for speed purposes. It also automatically discards
  articles with less than 50 revisions
- Non-random selection
  Given a title and language domain, the algorithm can get specific
  articles.
- Skip already-fetched articles
  This way we can 'refresh' our local copy of a history, or resume an
  interrupted fetch.

THINGS I DIDN'T IMPLEMENT

- Timestamp Limits
- leveraging 'diff'
- 

The final algorithm, with the above caveats, is as below:

   
Final algorithm for fetching discoverable revision history
------------

wiki = Wikipedia() //connection to wikipedia API
database = Database() //connection to database

depth = -1
if given_depth:
   depth = given_depth

corrupt = True
while corrupt:
      if given_title and given_language:
      	 page = wiki.request_page(given_title, given_language).json()
      else:
	 page = wiki.request_random_page().json() //return page details as json
      if valid(page):
      	 database.insert(page)
	 corrupt = False

parentid = page['parentid'] //this detail returned in json
visited = [parentid]
corrupted = []

while parentid != 0 and parentid is not in visited and depth != 0:
      if database.exists(revid):
      	 parentid = database.getparent(parentid)
      else:
	 page = wiki.request_by_revid(parentid).json()
      
         if valid(page):
      	    database.insert(page)
         else:
	    corrupt.append(parentid)

         parentid = page['parentid']
         visited.append(parentid)

      depth--;

for each id in corrupted:
      database.circumnavigate(id)      

------------
