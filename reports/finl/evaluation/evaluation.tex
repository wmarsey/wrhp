\chapter{Evaluation}

\section{Testing}

\subsection{Logging}

I log.

\subsection{Database}

\subsubsection*{Structure}

The database was written initially to take pageid and revids for
granted, but, on adding different domains to the program had to grow
to acomodate the lack of uniqueness in these values. The database has
grown organically with the project, and is robust. However, I believe
it's structure could be improved a little. The key dependencies are such: 

\begin{figure}
  \begin{math}
    P = Pageid\\
    T = Page title\\
    R = Revid\\
    D = Domain\\
    Cn = Content\\
    Un = Username\\
    Ui = Ui\\
    T = Revision timestamp\\
    S = Revision size\\
    Cm = Revision comment\\
    Mw = Math weight\\
    Cw = Citation weight\\
    Fw = Files / Images weight\\
    Lw = Link weight\\
    Sw = Structure weight\\
    Nw = Normal text weight\\
    G = Trajectory gradient\\
    Td = Trajectory distance\\
    F = Fetched flag\\
\\
    RD \rightarrow PMwCwFwLwSwNwCnCmGSTUnUiTd\\
\\
    PD \rightarrow TF\\
    \\
    UnD \rightarrow Ui\\
  \end{math}
  \caption{Database entities and key dependencies}
  \label{fig:dat-key}
\end{figure}

We can in figure~\ref{fig:dat-key} that the many of the entities of
the database rely on the $(revid, domain)$ key-pair, the only
exception being the $Username, Domain \rightarrow UserID$ and $Pageid,
Domain \rightarrow Title, Fetched flag$ relations. 

Comparing this to the database schema as they stand in
figure~\ref{database-schema} we see that some improvements could be
made immediately -- the $Username, Domain \rightarrow UserID$ relation
could recieve its own schema immediately, saving a lot of repeated
information in the `wikirevisions' table. This would also be useful as
the relation could easily be expanded upon on further study into
individual user habits, as I will suggest later.

Similarly, the `title' attribute of the `wikirevisions' table could be
moved to the `wikifetched' table, since it is unique to a
$(pageid,domain)$ pair. This would necessitate the addition of a
boolean column `Fetched' to the same table (the presence or lack of
presence of a given $(pageid,domain)$ pair in the `wikifetched' table
currently stands in place of that boolean value). However, given that
`wikifetched' is inevitably much smaller than `wikirevisions', the
space saved would be considerable regardless.

A further feature that can be done away with is the presence of two
revision IDs in the `wikitrajectory' table. This was useful in an
earlier implementation, where the code would manually check for both a
$(oldrevid,newrevid)$ \textit{and} $(newrevid,oldrevid)$ before
endeavouring to compute a new distance. This feature was scrubbed
early but the form of the table has existed to the end. Similarly,
content had its own table to facilitate quicker access to revision
content before the online-viewing feature was implemented in the
CLI. We could move content to the `wikirevisions' table, safely take
on the convention of the trajectory distance being unique to either
the parent or child ID, and reduce the `wikitrajectory' table by one
column.

Some structural inelegance aside, the database is fairly robust. But
given the above analysis we should recommend adoption of the schemata
described in ~\ref{fig:database-new}. Some separation of
figure~\ref{fig:dat-key}'s larger relation is recommended, merely for
sanity and maintenance's sake. If one of the distance calculations
goes awry, for instance, one would merely have to wipe one table and
recalculate.

\begin{figure}
  \label{fig:database-new}
  \centering
  \begin{subfigure}[b!]{0.3\linewidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      \underline{username} & \underline{domain} & userid\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{Table: wikicontent}
  \end{subfigure}
  \begin{subfigure}[b!]{0.3\linewidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      \underline{pageid} & \underline{domain} & title & fetched? \\
      \midrule
      $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{Table: wikipages}
  \end{subfigure}
  \begin{subfigure}[b!]{0.3\linewidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      \underline{revid} & \underline{domain} & distance\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikitrajectory}
  \end{subfigure}\\
  \vspace{10 mm}
  \begin{subfigure}[b!]{\linewidth}
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      \underline{revid} & \underline{domain} & pageid & username & time & size &
      comment & content \\ 
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$
      & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikirevisions}
  \end{subfigure}

  \begin{subfigure}[b!]{\linewidth}
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      \underline{revid} & \underline{domain} & maths & citations & filesimages & links &
      structure & normal & gradient\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &
      $\vdots$ & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikiweights} 
    \subref{weightstable}
  \end{subfigure}
  \caption{Recommended new schemata for storing wikipedia data}
\end{figure}

\subsubsection*{Integrity}

Since, as demonstrated above, we may rely on $(pageid,domain)$ and
$(revid,domain)$ pairs, we are able to embed these as native psql
primary key restraints. The database will throw error `23000,
integrity\_constraint\_violation' if this restraint is threatened by
the next transaction, which is passed up from the psycopg module as an
exception.\cite{psql-error}\cite{psyc-error} If such an error occurs,
we catch it, log it, and terminate the program cleanly.

In reality, though, these errors are infrequent. The code only
prepares and inserts a value if it cannot find it in the database, and
the insertion functions are implemented as to check before inserting
that it will not be duplicating any data.

\subsection{Data source}

Until fairly late in the project, storing large amounts of data in
local databases didn't create any notable problems. The PSQL database
used text compression as default on all string fields,\cite{psql-comp}
and the database was quick and responsive for most of the project.

However, towards the end of the project, the database was regularly
filled with data relating to ~300,000 revisions, and under this load
began to lag. We also employed ClusterSSH in order to run the program
on multiple machines,\cite{clusterssh} distributing the HTTP requests
to wikipedia and building a large database, but putting the database
under a considerable burden.

