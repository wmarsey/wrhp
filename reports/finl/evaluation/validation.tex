\section{Machine learning validation attempts}
\label{mlisbad}
We attempted to validate our model using machine learning. Given the
form of the model's output -- a 1D array of numbers -- machine
learning seemed to be a natural choice.

We were interested to see if we could predict gradient factor. Since
the gradient factor relates closely to whether or not the edit was
included in the final version, we were interested if with machine
learning could possibly predict the gradient factor given the weights
that we calculate. We could possibly see if the text analysis data we
had collected would be able to similarly measure affects on the
article.

We used the scikit-learn python package to approach the
problem,~\ref{scikit-learn} working with the ~180,000 test cases (i.e.
complete revision records) extant in the database at the time. We
tried predicting gradient factor with the following forms of training
data:

\begin{itemize}
  \item \textbf{Weights}\\
    Simply the calculated weights, as found in the database weight table. 

  \item \textbf{Weights and size change}\\
    The calculated, along with information about whether the revision
    made the article larger or smaller

  \item \textbf{Weights size change, time change}\\
    The calculated, along with information about whether the revision
    made the article larger or smaller

  \item \textbf{Weights summed to one value, and size change}\\
    Summing the weights to one value is roughly equivalent to taking a
    plain levenshtein distance of the article on the whole, rather
    than one separated by species.
    
  \item \textbf{Summed weights and user edit count over
    domain}\\ We take the generic weight and combine it with the
    user's activity over the whole wikipedia domain.
    
  \item \textbf{Summed weights and user edit count over
    article}\\ Similar to above, but only counts user activity in the
    same article.

\end{itemize}

We experimented with various types of regression techniques, but our
results were not very good, with the regression function rarely
fitting to the data with any accuracy. The validation scripts can be
run from validator.py, in the validation subfolder.

Since the gradient factor is sensitive to time, and the scale of that
time, we also tried seperating the data into two categories,
'towards-final' as 1 and 'away-from-final' as 0, by simply rounding
the gradient factor to it's nearest integer, and repeated the same
tests as detailed above, but similarly the classification was not very
accurate.

We identify two problems that may have effected this. 

Our first is perhaps the incompleteness of our data. In particular,
the incompleteness of outr understanding of the text itself. It was a
central premise of this study that we were to make no attempt at
understanding the actual content of the text.

But perhaps the actual fact content of the text is quite important,
particularly on an encyclopedia, to its survival --- perhaps Wikipedia
is predictable in this regard, that facts have an innate discoverable
quality. Perhaps in this case we would be in a better place to predict
success if we could measure the factual accuracy of new text, run a
spell checks on it, etc.

Our second problem may be that, perhaps, much of the data we analysed
actually doesn't greatly effect the survival rate of the edit. Much of
the information lost from and gained by pages like Rupert Sheldrake's,
or Derek Smart's was factually correct, and not necessarily
malformed. The reason for the exclusion of content was to quell a
conflict in the case of the former text, whereas in the latter page it
was the personal opinions of individual Wikipedia editors about which
facts actually \text{were} facts that caused (and continue to cause)
complications.

Talk page data is one thing, and we looked briefly at how the talk
page data can correlate with article data, but external context is
relevant too. Rupert Sheldrake's interview on the BBC coincides with
changes in his article, and we can refer to Lih's 2004 article on
edits made immediately after celebrity deaths.

We note that there are less regular editors on Wikipedia now than
there was in 2007, user-count `has shrunk by more than a third since
[then] and is still shrinking'.\cite{wiki-decline} Wikipedia's `formal
mechanisms for normal articulation are shown to have calcified against
changes â€“-- especially changes proposed by newer
editors'.\cite{wiki-decline-2} The study identifies, amongst other
things, a number of tools, bots and policies that have often tended to
revert entries regardless of their content.

But Wikipedia's internal problems are of not real issue with this
study, they only give us reason to doubt whether we could use a simple
dataset in order to characterise the complex conditions that conduce a
successful contribution to Wikipedia.

Rather, the lesson we learn here is simply that a wide context is
neccesary for understanding and characterising real-world human
collaboration.
