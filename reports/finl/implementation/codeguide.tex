
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[align=center,node distance=1cm]

    \node (database)[dtb, fill=col1]{Database};

    \node (wikidata)[bx, fill=col2, above=of database]{WikiDatabase};

    \node (wikifetch)[bx, fill=col2, above=of wikidata]{WikiRevisionScrape};

    \node (wikilaunch)[bx, fill=col2, left=0.5\mwidth of wikifetch]{WikiLaunch};

    \node (analysis)[bx, fill=col2, right=0.5\mwidth of wikifetch]{WikiAnalysis};
        
    \node (datplot)[bx, fill=col2,  right=0.5\mwidth of analysis]{WikiDataPlot};

    \node (datahandle)[bx, fill=col2, below= of datplot]{WikiDataHandle};

    \node (wikicli)[bx, fill=col5, above= 3cm of wikifetch]{WikiCLI}; 
    
    \node (wikipedia)[cld, fill=col4, left=of wikicli]{\textit{Wikipedia}};

    \draw [-] (database) -- (wikidata); 

    \draw [-] (wikidata) -- (analysis); 

    \draw [-] (wikidata) -- (wikifetch); 

    \draw [-] (wikidata) -- (datahandle); 

    \draw [-] (wikifetch) -- (wikipedia); 

    \draw [-] (wikilaunch) -- (wikipedia); 

    \draw [-] (datahandle) -- (datplot);

    \draw [-] (datplot.north) -- (wikicli); 

    \draw [-] (wikifetch) -- (wikicli); 

    \draw [-] (wikicli) -- (analysis); 

    \draw [-] (wikicli) -- (wikilaunch);      

    %% \node (blob) [
    %%   draw=col5, 
    %%   fit= (wikidata) (datplot) (wikifetch), 
    %%   inner sep=0.4cm,
    %%   thick,
    %%   rounded corners=8mm
    %% ] {};

    %% \node [
    %%   col5, 
    %%   anchor=south east, 
    %%   xshift=-4mm,
    %%   yshift=4mm
    %% ] at (blob.south east) {\textbf{the WikiRevision package}};

  \end{tikzpicture}

  \caption{Diagram showing the connections between entities in python implementation}
\end{figure}

\section{Guide to the code}

The project was implemented in Python, with a small C++ core for the
Levenshtein distance implementation. The different classes are fairly
independent, but share the WikiData class, and are coordinated by the
CLI classes. 

To compute Levenshtein distance we employ a C++ script that returns
Python-variables, and is compiled into a shared object library for use
in Python. The Levenshtein distance calculations can be quite slow
with long strings, particularly when calculating trajectory as the
most recent revision can often be the largest, and we run the
algorithm on that each time. Pair-distance can be a little quicker
generally, and we are able to implement some multi-processing because
of the string-splitting.

Otherwise, the code runs reasonably quickly, with it's speed most
often limited by pthe HTTP requests, and the speed of the
database.

This majority of this software requires PSQL and the psycopg2 package
to run. The machine-learning script requires numpy and sci-kit learn.

Here we discuss the five main classes in the project, and their
function in this project. Appendix~\ref{sec:dirtree} details the
file structure of the code base.

\subsection*{WikiData}
The WikiData class interfaces with the database, and maintains its
integrity. It is importable as a package by the root-folder functions
by the command `import database'.

The functions here are as simple as possible, mainly basic fetches,
inserts, with some alter functions. Each function has the same plan:
check if necessary that the operation is needed, collect SQL string,
collect data for string substitution, try to execute, if the execute
worked, get the results, if the results are well-formed, return them in the
correct format.

The functions towards the bottom of the file are dump functions,
performing more complex operations for the plotting and machine
learning scripts discussed below. They are harder to maintain, but are
much more efficient than executing a combination of the other
functions.

\subsection*{DHandler}
This code fetches from the WikiData class, and arranges the results
into plottable values for WikiDataPlot.

\subsection*{wikiDataPlot}
The wikiDataPlot file can be run as a script, or instantiated as a
class. 

Running as a script accesses the dump-plot functions, fetching and
plotting a lot of data from the database at once. It is controlled by
a few command line arguments which are apparent in the code. This
mass-plotting was really only implemented to help the writing of this
report.

Otherwise, the lineGraph, barChart and trajectoryGraph functions are
self-explanatory. These files will be output from the CLI given the
\textit{-p} option, and inserted either into the default file
location, or a folder specified at the command line.

\subsection*{wikiFetch}
This class implements algorithm~\ref{dist-calc}, including all the
necessary logical extensions to deal with CLI parameters. It
interfaces with Wikipedia through various language sub-domains. It will
attempt to fetch one article, trying until it succeeds if asked to
choose a random article. Once it has found a file, details of the
fetched page can be queried using function like getPageID, etc.

The class automatically disregards any article with less than 50
revisions, as we found analyses more worthwhile with longer
histories. This can be overridden on instantiation of the class. 

The language sub-domains are kept in a .csv file, scraped from the
English Wikipedia, from which it picks and checks
domain name from that if asked to.

The search function also attempts to suggest a title to the user if a
page request fails, (though the suggestions are often pretty
inappropriate...) using Wikipedia API functionality. 

\subsection*{WikiAnalysis}
An implementation of the
algorithms~\ref{dist-calc}-\ref{traj-calc}. We employ multiprocessing
to speed the computation up on multi-core systems, computing
levenshtein each separate species of text in a separate process --
each of the different edit distances calculated in the process in
figure~\ref{fig:split-diff} is given it's own. 

Conceptually, the child processes are spawned at
line~\ref{mprocess-spawn} of algorithm~\ref{dist-calc}, and the
results collected just before line~\ref{mprocess-return}. As shown in
figure \ref{fig:mprocessing} the `normal' text portion comparison is
inevitably longer than any non-normal, and we execute this thread last
(it being the remainder text), so we are mostly waiting on this
process for a while before the computation can proceed. Nevertheless,
we found a notable increase in performance by implementing
multiprocessing.

\begin{figure}
  \centering
  \begin{tikzpicture}[x=0.5cm, y=0.5cm]
    \draw[step=.1cm,gray,very thin] (0,6) grid (30,5);
    \draw[step=.1cm,gray,very thin] (0,4) grid (30,3);
    \draw[step=.1cm,gray,very thin] (0,2) grid (30,1);
    \draw[step=.1cm,gray,very thin] (0,0) grid (30,-1);
      
    \draw[dashed, navyblue] (2,1) -- (2,0);
    \draw[dashed, navyblue] (5,3) -- (5,0);
    \draw[dashed, navyblue] (8,5) -- (8,0);
    \draw[dashed, navyblue] (10,1) -- (10,0);
    \draw[dashed, navyblue] (9,3) -- (9,0);
    \draw[dashed, navyblue] (27,5) -- (27,0);

    \draw[fill, red] (0,0) rectangle (2,-1);
    \draw[fill, red] (3,0) rectangle (5,-1);
    \draw[fill, red] (6,0) rectangle (8,-1);
  
    \draw[fill, blue] (2,2) rectangle (10,1);
    \draw[fill, blue] (5,4) rectangle (9,3);
    \draw[fill, blue] (8,6) rectangle (27,5);

    \draw[fill, beige] (27,0) rectangle (30,-1);


    \draw(a)[fill, red] (0,-2) rectangle (4,-2.5);
    \node[font=\small] at (4,-3) {Regex splitting operations};

    \draw(b)[fill, blue] (11,-2) rectangle (15,-2.5);
    \node[font=\small] at (14.5,-3) {Levenshtein calculations};

    \draw(c)[fill, beige] (22,-2) rectangle (26,-2.5);
    \node[font=\small] at (24.8,-3) {Database insertion};

    \node[font=\small, align=left] at (2,6.35) {Child process 3};
    \node[font=\small, align=left] at (2,4.35) {Child process 2};
    \node[font=\small, align=left] at (2,2.35) {Child process 1};    
    \node[font=\small, align=left] at (2,0.35) {Parent process};
    
  \end{tikzpicture}
  \caption{Diagram showing multi-processing in pair distance
    calculation}
  \label{fig:mprocessing}
\end{figure}

\subsection*{The Database}
The database is implemented in PSQL, and accessed via our python
package, `wikiDatabase'. The package leverages the psycopg library,
which allows simple access to the SQL database via
python.\cite{psycopg2} The package is used by all the other classes
used in this project and provides simple inserting, changing, fetching
and checking of data.

Some more complex operations are undertaken by the class, mainly
preparing large `data-dumps' for mass-plotting (in preparation for this
report) and validation exercises. These functions were written for
performance purpose (the data they deliver could easily have been
fetched using the other functions, but would have taken much longer),
and are clearly marked in the code as separate from the basic fetch,
get and insert functions. 

The database relies upon the uniqueness of revision ID and
domain-name, and page ID and domain-name pairs.  pairs to
function. The existing database schemata can be found in
figure~\ref{database-schema}. We talk about improving this schemata in
the evaluation chapter.

\begin{figure}
  \vspace{10mm}
  \centering
  \begin{subfigure}[t]{0.3\linewidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      \underline{revid} & \underline{domain} & content\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{Table: wikicontent}
  \end{subfigure}
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \begin{tabular}{cc}
      \toprule
      \underline{pageid} & \underline{domain} \\
      \midrule
      $\vdots$ & $\vdots$\\
    \end{tabular}
    \caption{Table:\newline wikifetched}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      \underline{revid1} & \underline{revid2} & \underline{domain} & distance\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikitrajectory}
  \end{subfigure}\\
  \vspace{10 mm}
  \begin{subfigure}[b!]{\linewidth}
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      \underline{revid} & \underline{domain} & pageid & title & username & userid & time & size &
      comment \\ 
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$
      & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikirevisions}
  \end{subfigure}\\
  \vspace{10mm}
  \begin{subfigure}[b!]{\linewidth}
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      \underline{revid} & \underline{domain} & maths & citations & filesimages & links &
      structure & normal & gradient\\
      \midrule
      $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &
      $\vdots$ & $\vdots$ & $\vdots$ \\
    \end{tabular}
    \caption{Table: wikiweights} 
  \end{subfigure}
  \caption{Schemata for the database used to store wikipedia data}
  \label{database-schema}
\end{figure}

A database backup can be found in dbdump.sql, and a script for setting
up the schema in schema.sql.

\clearpage

\subsection*{wikiLaunch}
A simple collection of functions for launching Wikipedia in the
browser, able to target diffs between two revisions, old revisions,
pageids, and user pages and all Wikipedia sub-domains.

\subsection*{The CLI interface}
The CLI can be accessed using the short-cut wrhp (Wiki Revision History
Portal), and offers a limited, automated coordinating and run of
the above classes. It can be manipulated using the arguments as follows:

\begin{itemize}[label={}]
  \item \textbf{Default behaviour.} Fetches a random page from a
    random Wikipedia to the database, and analyses it.
  \item \textbf{-s} Scrape only. 
  \item \textbf{-p} Plot data. Saves png files to location given by
    the --plotpath argument.
  \item \textbf{-i} Open the interactive plot window for a given (or
    random) article once analysed.
  \item \textbf{-v} View a Wikipedia page online. Must be used with
    --domain. Can be used to view a diff (using --revid and
    --oldrevid), a specific revision (using only --revid), or the
    latest version of a page (using --pageid).
  \item \textbf{-t} `Trundle' mode. Repeats the given operation until
    interrupted. Useful for building up a database. Cannot be used
    with the --titles argument.
  \item \textbf{-S[S]} `Silent' mode. One `S' silences stdout, two
    silences both stdout and stderr.
  \item \textbf{--title [\textit{str}]} Specify the
    pages to be scraped. Must be used with --domain. Case (and
    spelling...)  sensitive.
  \item \textbf{--domain [\textit{str}]} Specify the domain to connect
    to. May be used without --titles, limiting the random page pick to
    one domain. Must be the short version (`en', `de', etc.).
  \item \textbf{--scrapemin [\textit{int}]} Specify the minimum amount of
    pages to be scraped for one page. Default = 50.
  \item \textbf{--plotpath [\textit{filepath}]} Specify location for
    plots to be stored. Default = .
  \item \textbf{--revid} Specify the revision ID to be viewed. Used
    with -v.
  \item \textbf{--oldrevid} Specify the old revision ID when viewing a
    diff. Used with -v.
  \item \textbf{--pageid} Specify a target pageid. Can be used in -v
    and -s.
\end{itemize}
