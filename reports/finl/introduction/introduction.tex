\chapter{Introduction}

\section{Motivation and Objectives}

The dynamics of mass online collaboration is of much academic
interest.

Despite early skepticism (particularly concern over the
inherent chaos in the system: ``...edits, contributed in a
predominantly undirected and haphazard fashion by ... unvetted
volunteers.''\cite{Wilkinson2007}), it is widely claimed to be a
success, `the best-developed attempt thus far of the enduring quest to
gather all human knowledge in one place'\cite{Mesgari2014}.


Wikipedia, the platform in question here, is only around a
decade old, but in that time it has become a pre-eminent source of
knowledge one the internet. It is one of the most visited sites in the
world, and the most visited reference site by far. It is a huge site,
with over 4.5 million article, in around 300 language languages, and
is completely open source and free-of-charge. It is an unparalleled
data source, detailed and easy to access.

However, one of the major factors contributing to the proliferation of
academic work on wikipedia is suspicion. Investigations into the worth
and quality of Wikipedia articles abound. Work in this area ranged
from popular science to production software to full-blown academic
theses, and until around 2010 continued to be a well-visited topic.

In recent years, however, the research into wikipedia's extrinsic
quality has waned. These studies peaked at around 2007, in the release
of the Wikitrust software, which brought together much of the
preceding researh on the subject, and is discussed later. Since then,
though, papers on Wikipedian quality became less frequent, and
Wikipedia reached real omnipresence.  In these years, while the fears
that these crowd-sourced articles would supercede established
reference tomes were certainly realised, so did the public seem to
maintain a healthy suspicion of the site. Wikipedia is a central point
of fact-checking, with even search tools such as Google's voice search
and Apple's Siri taking it's content for granted, serving up
Wikipedian content automatically after voice searches, but the site is
still not accepted as a citable source.

Criticisms of Wikipedia still exists, however. Though, whilst they
began as investigations into external quality, now they more often
than not concern Wikiepdia's internal health. We can no longer argue
about whether or not the public should be reading these articles --
they already are. Instead, we examine the internals of Wikipedia, and
into the community that is involved in its growth. 

This is the context for our study. We can assume that an article has
worth, we don't care what that might be. We look at how this
may be distributed within the article. What can we find out about the
article by examining it's construction? What can we automatically
measure of a revision's text?

The objective of this project was to find a fair and concise way to
analyse a Wikipedia revision, both in terms of the text operations,
and in terms of an edits significance in the overall history of the
article.

\section{Contributions}

Technique for fetching an article's whole revision history, using the
API. Also, thorough investigation into the nature of this data - the
commonly corrupt areas, the natures of different language wikipedias,
etc.

A more nuanced way to look at undone revisions, building on the
Wikitrust work. 

\section{Statement of Originality}

As stated above, a lot of work has been done on the tangential study
of article quality, and a lot of that work has been useful here, and
is credited where

This project was also undertaken for the MSc degree at Imperial in
2013. That project has been taken into account, but has been expanded
upon enough to not warrant mention. There was not a lot that could be
used from the previous report.

For this project I have produced:
\begin{itemize}
\item An Python wrapper for the Wikipedia Revisions API (separate from
  existing modules)
\item A series of classe which leverage the above module, systematically
  fetching, sanitising, storing Wikipedia revision
  histories, as well as  This script wraps a C++ implementation
  of a Levenshtein-distance algorithm.  
\item A command line tool for configuring and running all of the above. 
\item A validation script which fetches leverages machine learning techniques
 in order to validate the produced model
\end{itemize}
