\chapter{Introduction}

\section{Motivation and Objectives}

The dynamics of mass online collaboration is of much academic
interest. Not only for their complexity, but their sheer
newness. Wikipedia as a platform is only around a decade old, but in
that time it has become the pre-eminent source of knowledge in the
internet-using world, one of the most visited sites in the world, and
the most visited reference site by far. However, the admiration
expressed towards the site is tempered by equal suspicion.

Accordingly, academic investigations into the worth and quality of
Wikipedia articles abound. Work in this area, reaching its peak at
around the end of the 00s, is detailed and wide ranging (if a little
out of date). In recent years, however, the research efforts have
waned slightly. It has been over this time that Wikipedia reached its
real omnipresence, and while the fears that these crowd-sourced
articles would supercede established reference tomes were realised, so
did the public seem to maintain a healthy suspicion of the site.

Instead, critics have begun to concenterate on the sites
internals. Polemics on the the articles' external worth have become
moot -- the site is now fully in the spotlight, google automatically
fetches and serves data from it, it is one of the highest-rating
search matches for most general subjects. We can no longer argue about
whether or not the public should be reading these articles -- they
already are. Instead, we look to Wikipedia internals.

This is the context for our study. We can assume that an article has
worth, we don't care what that might be. Instead we look at how this
may be distributed within the article. What can we find out about the
article by examining it's construction? What can we automatically
measure of a revision's text?

\section{Contributions}

Contributions.


\section{Statement of Originality}

As stated above, a lot of work has been done on the tangential study
of article quality, and a lot of that work has been reshaped and
applied here, and will be credited.

This project was also undertaken for the MSc degree at Imperial in
2013. That project has been taken into account, but has been expanded
upon enough to not warrant mention. There was not a lot that could be
used from the previous report.

For this project I have produced:
 - An Python wrapper for the Wikipedia Revisions API (separate from
 existing modules)
 - A series of classe which leverage the above module, systematically
 fetching, sanitising, storing Wikipedia revision
 histories, as well as  This script wraps a C++ implementation
 of a Levenshtein-distance algorithm.  
 - A command line tool for configuring and running all of the above. 
 - A validation script which fetches leverages machine learning techniques
 in order to validate the produced model
