\section{Objectives}
Our principal aim was to design a series of procedures for
distributing a `share' of a collaborative article to each of that
article's contributors. Traditionally these shares are negotiated on a
case-by-case basis, through individual negotations, or according to
balances struck in previous battles. Wee discuss this in further
detail on page~\pageref{sec:copyright}.

Our investigation instead looks to automatic share distribution in the
environment of many-user on-line collaboration; how may we
automatically define a individual user's stake in a collaborative
work?

Wikipedia is the natural choice for the study, though defining user
`ownership' in this particular context seems quite unnatural. The
website that bills itself as the `free encyclopaedia that anyone can
edit' can hardly have any obligation to its users to define a sense of
individual ownership of the article, whole or
otherwise.\cite{wiki-copyr} However, as an intellectual exercise, the
question is very relevant, with on-line platforms increasing in
popularity in education,\cite{wiki-collab} and the
workplace,\cite{workplace-future} and generally becoming a `necessity'
in day-to-day life.\cite{alsurvey}

Thankfully, the technological platforms that allow projects such as
Wikipedia to grow also facilitate their study. Using the technology,
we may may examine how to describe the nature of an individual
contribution amongst many.. Wikipedia is, after all, one of the most
popular sites on the internet, and certainly one the most popular
reference site on the internet (see page~\pageref{sec:popularity}).

On a more practical level, Wikipedia represents a free, flexible and
(fairly)\footnote{The API proved to be quite unreliable. This is
  discussed on page~\pageref{sec:wiki-api}.} complete data set with which to experiment analysing text
change over long periods. From this base work we can begin to approach
analysing more specific data sets, such as source-control platforms
like Github.

Another benefit of analysing Wikipedia is that interactions between
users are fairly completely mediated by the Wikipedia software, and
evidenced in exactly the same way the actual data is, allowing us to
examine a relatively wide range of site properties using a similar set
of tools.

For this project, we produce a system for automatically downloading an
article's history, along with various methods with which to process
and describe that data in a meaningful way. We use a Levenshtein
edit-distance calculating algorithm in order to examine the magnitude
of change between edits, as well as to examine the context of those
edits.

For each revision we calculate a set of integers and real numbers that
represent a multi-dimension characterisation of the change that edit
caused in the article. We provide other means to express the degree to
which that edit was significant in terms of the history of that
article thus far.

\section{Contributions}

The major contribution here is a new way of visualising the article's
history, and a technique for translating that into a meaningful,
real-numbered value for each revision.

We also discuss using this technique describe various other contexts
that come to bear upon journey of each article. Our main technique
involves calculating the similarity of every revision to the final
version. However, using the same software we may also track the
history of the talk page for each article alongside the article
itself, as well as test for the existence of `arbitration requests'
and other artefacts of Wikipedian bureaucracy that may shape an
article's growth.

The software produced here automates the analytical procedures we
outline in chapter~\ref{chap:method}, as well as providing a modular
framework that may be modified for further study.

\section{Originality}
There has been a lot of studies tangential to the topics we discuss
here -- as we'll cover in chapter~\ref{ch:background}, Wikipedia is a
hotbed of academic study in many disciplines -- but much of the
similar work to this works to different ends, and therefore a lot of
the work here is original work.

The most related work is that of WikiTrust software. It was developed
in the context of criticism's of Wikipedia's factual integrity, and
aimed to rate the the editors of each article, transforming each
article into a kind of heat map or trustability, highlighting words
from more trusted editors more intensely than
others.\cite{adler2012wikitrust} This study has been evaluated as
ineffective, however.\cite{Lucassen2011}

In this project we use wikipedia as a model for automatically
discovering appropriate measures of collaborative share. We are less
concerned about `quality' in any arbitrary, external sense. Rather we
are interested how we may define quality using the qualities of the
history itself. That said, a lot of the studies are very interesting
in light of our work. One of the techniques used in WikiTrust is very
important in our conception of `trajectory'. We discuss this more on
page~\pageref{sec:wikipedia}.

We offer less judgements on quality in this project, and our aims are
more general, but some of the techniques resurface here. In
particular, we modify the WikiTrust method for identifying undone work
to create a much more detailed flow of change.

This project was also undertaken for the MSc degree at Imperial in
2013. That project has been taken into account, but as it contained no
real developments upon existing studies, has been expanded upon
greatly here.
